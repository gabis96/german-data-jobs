{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape jobs from LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests \n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aim: Retrieve all current data jobs in Germany\n",
    "url = 'https://de.linkedin.com/jobs/search?keywords=data&location=Deutschland&geoId=101282230&trk=guest_homepage-basic_jobs-search-bar_search-submit&position=1&pageNum=0' # last month\n",
    "url = 'https://de.linkedin.com/jobs/search?keywords=data&location=Deutschland&geoId=101282230&sortBy=R&f_TPR=r86400&trk=guest_homepage-basic_jobs-search-bar_search-submit&position=1&pageNum=0' # last 24hours\n",
    "\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of Soup with the response body and html parser. Soup will be used to process the html\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test soup and get number of jobs found\n",
    "nresults = soup.find('span', class_='results-context-header__job-count').text.strip()\n",
    "nresults = int(nresults.replace('.', ''))\n",
    "nresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Healthcare Data Analyst - vitagroup - Mannheim - https://de.linkedin.com/jobs/view/healthcare-data-analyst-at-vitagroup-3467828706?refId=pR5QXAPn%2Bgl%2FHrDVKB9OCA%3D%3D&trackingId=P3MWghXLlqwcZ9wcw7ULcg%3D%3D&position=1&pageNum=0&trk=public_jobs_jserp-result_search-card'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Selectors\n",
    "job_title = soup.find('h3', class_='base-search-card__title').text.strip()\n",
    "job_company = soup.find('h4', class_='base-search-card__subtitle').text.strip()\n",
    "job_location = soup.find('span', class_='job-search-card__location').text.strip()\n",
    "job_url = soup.find('a', class_='base-card__full-link')['href']\n",
    "\n",
    "f'{job_title} - {job_company} - {job_location} - {job_url}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After playing with the url to figure out the parameters and how to retrieve the dynamic jobs loaded when scrolling and pagination, it can be concluded:\n",
    "- position: is not relevant\n",
    "- start: indicates the index of the firt post to show, or the delta. As in each call it retrieves 25 jobs, calls should be setting start+=25.\n",
    "- pageNum: indicates the pagination, which changes every 1000 jobs. Therefore it goes up one unit every time start=1000, and start is reset back to 0.\n",
    "\n",
    "The url follows the next pattern:\n",
    "https://de.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=data&location=Deutschland&geoId=101282230&trk=guest_homepage-basic_jobs-search-bar_search-submit&position=1&pageNum=0&start=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986\n"
     ]
    }
   ],
   "source": [
    "# Modified urlfound in the API calls\n",
    "url = 'https://de.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=data&location=Deutschland&geoId=101282230&trk=guest_homepage-basic_jobs-search-bar_search-submit&position=1&' # past month\n",
    "url = 'https://de.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=data&location=Deutschland&geoId=101282230&sortBy=R&f_TPR=r86400&trk=guest_homepage-basic_jobs-search-bar_search-submit&position=1&' # past 24hours\n",
    "url = 'https://de.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=data&location=Deutschland&geoId=101282230&sortBy=R&f_TPR=r86400&trk=guest_homepage-basic_jobs-search-bar_search-submit&position=1&pageNum=0&start=0' # past 24hours\n",
    "\n",
    "def get_jobs(url, nresults):\n",
    "    jobs = []\n",
    "    page_num = 0\n",
    "    start_num = 0\n",
    "    \n",
    "    while len(jobs) < nresults:\n",
    "        page = url + f'pageNum={page_num}&start={start_num}'\n",
    "        response = requests.get(page)\n",
    "\n",
    "        if response.status_code == 404:\n",
    "            break\n",
    "\n",
    "        # Get current page jobs (step=25)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser') \n",
    "        page_jobs = soup.find_all('div', class_='base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n",
    "        jobs.extend(page_jobs)\n",
    "\n",
    "        # Update params\n",
    "        start_num += 25\n",
    "        if start_num == 1000: \n",
    "            start_num = 0\n",
    "            page_num += 1\n",
    "\n",
    "            print(len(jobs))\n",
    "\n",
    "        time.sleep(2) # too many calls in a short time provoked soup not finding the tags or not parsing well.\n",
    "\n",
    "    return jobs\n",
    "\n",
    "jobs = get_jobs(url, nresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jobs(jobs):\n",
    "    data = []       # jobs\n",
    "    error = []      # jobs unable to process\n",
    "\n",
    "    for i, job in enumerate(jobs):\n",
    "        try :\n",
    "            data.append({\n",
    "                'Title': job.find('h3', class_='base-search-card__title').text.strip(),\n",
    "                'Company': job.find('h4', class_='base-search-card__subtitle').text.strip(),\n",
    "                'Location': job.find('span', class_='job-search-card__location').text.strip(),\n",
    "                'Link': job.find('a', class_='base-card__full-link')['href']\n",
    "            })\n",
    "        except:\n",
    "            # process jobs that rise error\n",
    "            error.append(job)\n",
    "            print(f'There was a problem scrapping job {i}')\n",
    "\n",
    "    return data, error\n",
    "\n",
    "data, _ = process_jobs(jobs) # realizing error always comes empty so no need to process it, if it happens is ok to skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Healthcare Data Analyst</td>\n",
       "      <td>vitagroup</td>\n",
       "      <td>Mannheim</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/healthcare-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Staffbase</td>\n",
       "      <td>Dresden</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/data-enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Staffbase</td>\n",
       "      <td>Chemnitz</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/data-enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Staffbase</td>\n",
       "      <td>Leipzig</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/data-enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>aparkado</td>\n",
       "      <td>Deutschland</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/data-scienti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>Red Bull HQ Graduate Programme (based in Salzb...</td>\n",
       "      <td>Red Bull</td>\n",
       "      <td>Berlin, Deutschland</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/red-bull-hq-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>Data Engineer - Marketing</td>\n",
       "      <td>Cititec</td>\n",
       "      <td>Berlin, Deutschland</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/data-enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>Business Analyst (m/w/d)</td>\n",
       "      <td>Arctic Wolf</td>\n",
       "      <td>Frankfurt am Main</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/business-ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>Web / Digital Analyst Business Intelligence (m...</td>\n",
       "      <td>zipmend GmbH üöõüí® Express, LTL &amp; FTL Trans...</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/web-digital-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>Data Analyst - Capacity Planning (m/f/d)</td>\n",
       "      <td>Flink</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/data-analyst...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1011 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "0                               Healthcare Data Analyst   \n",
       "1                                         Data Engineer   \n",
       "2                                         Data Engineer   \n",
       "3                                         Data Engineer   \n",
       "4                                        Data Scientist   \n",
       "...                                                 ...   \n",
       "1006  Red Bull HQ Graduate Programme (based in Salzb...   \n",
       "1007                          Data Engineer - Marketing   \n",
       "1008                           Business Analyst (m/w/d)   \n",
       "1009  Web / Digital Analyst Business Intelligence (m...   \n",
       "1010           Data Analyst - Capacity Planning (m/f/d)   \n",
       "\n",
       "                                                Company             Location  \\\n",
       "0                                             vitagroup             Mannheim   \n",
       "1                                             Staffbase              Dresden   \n",
       "2                                             Staffbase             Chemnitz   \n",
       "3                                             Staffbase              Leipzig   \n",
       "4                                              aparkado          Deutschland   \n",
       "...                                                 ...                  ...   \n",
       "1006                                           Red Bull  Berlin, Deutschland   \n",
       "1007                                            Cititec  Berlin, Deutschland   \n",
       "1008                                        Arctic Wolf    Frankfurt am Main   \n",
       "1009  zipmend GmbH üöõüí® Express, LTL & FTL Trans...              Hamburg   \n",
       "1010                                              Flink               Berlin   \n",
       "\n",
       "                                                   Link  \n",
       "0     https://de.linkedin.com/jobs/view/healthcare-d...  \n",
       "1     https://de.linkedin.com/jobs/view/data-enginee...  \n",
       "2     https://de.linkedin.com/jobs/view/data-enginee...  \n",
       "3     https://de.linkedin.com/jobs/view/data-enginee...  \n",
       "4     https://de.linkedin.com/jobs/view/data-scienti...  \n",
       "...                                                 ...  \n",
       "1006  https://de.linkedin.com/jobs/view/red-bull-hq-...  \n",
       "1007  https://de.linkedin.com/jobs/view/data-enginee...  \n",
       "1008  https://de.linkedin.com/jobs/view/business-ana...  \n",
       "1009  https://de.linkedin.com/jobs/view/web-digital-...  \n",
       "1010  https://de.linkedin.com/jobs/view/data-analyst...  \n",
       "\n",
       "[1011 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe with records\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://de.linkedin.com/jobs/view/data-engineer-at-staffbase-3473357475?refId=vttpEyVwQApXk9Dhk8V2PQ%3D%3D&trackingId=EUFWDd2MpvdqtroF9e1kOQ%3D%3D&position=2&pageNum=0&trk=public_jobs_jserp-result_search-card\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\r\\xb1\\xa7\\xfbhWA\\xf3\\xfa\\x9b\\xa9Ur\\xd7\\xb1:\\t\\xfe T\\x9bu\\xde\\xacV`\\x7f\\x00r]\\x9b\\x9c\\xdb\\\\\\xa6\\x1f\\xf4\\xdb\\x1c\\x12OW\\xa6g\\x031;\\xa3\\x1c']\n",
      "Bad pipe message: %s [b'i\\xc6B\\x90BG\\xef{\\xb8\"\\xec\\xea\\xa4\\x93%\\x16\\xa8=\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0\\'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00']\n",
      "Bad pipe message: %s [b'\\xff\\x01\\x00\\x00']\n",
      "Bad pipe message: %s [b'\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\\x02\\x04\\x02\\x05\\x02\\x06\\x02']\n",
      "Bad pipe message: %s [b'I\\xd6F\\rq\\x0c\\xd4/\\xb4t\\x91\\xe7b\\xd0\\xe9U\\xe2\\xdd\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R']\n",
      "Bad pipe message: %s [b\"\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\"]\n",
      "Bad pipe message: %s [b'\\x05\\x08\\x06']\n",
      "Bad pipe message: %s [b'\\x05\\x01\\x06', b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'V\\xc2)\\xf8\\xd6\\xebl\\x15\\xa4s\\xfa\\xe1O4\\xb3\\xb2\\xa3Q\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n']\n",
      "Bad pipe message: %s [b'\\x9et\\xe8\\xe2\\xb6\\x92\\xe4r6\\xe5\\xf3\\x1e\\x05\\xec\\xfc\\x9a!d\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00']\n",
      "Bad pipe message: %s [b'\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\n",
      "Bad pipe message: %s [b'\\rQ\\xd1\\x05\\xb8 \\x02\\txp\\x05e}\\x9c&\\x07\\x87\\xac\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04']\n",
      "Bad pipe message: %s [b'\\xe3KN\\xfc\\x1e\\xb3\\xbb\\xc1\\xc0\\x98\\x8d\\x0e^\\x14]\\xbd\\xe5|\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00']\n",
      "Bad pipe message: %s [b'C:z}\\xb9\\x8b\\xc6z\\xb9\\xaaN\\x9b\\xec\\xf4\\xda+\\x8c:\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04']\n",
      "Bad pipe message: %s [b'fX\\xca\\xdeP@\\xd7\\x12\\xd2a6E\\xf2,\\x99n\\x82\\xae\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0']\n",
      "Bad pipe message: %s [b'\\x9d\\x00=\\x00']\n",
      "Bad pipe message: %s [b\"/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00\"]\n",
      "Bad pipe message: %s [b'\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0']\n",
      "Bad pipe message: %s [b'\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00', b' \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05']\n",
      "Bad pipe message: %s [b'\\x03', b'\\x04\\x02\\x04', b'\\x01\\x03', b'\\x03', b'\\x02', b'\\x03']\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[1, 'Link'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape job post text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = df.Link\n",
    "\n",
    "def scrape_text(links):\n",
    "    texts = []\n",
    "\n",
    "    for link in links:\n",
    "        response = requests.get(page)\n",
    "        print(response)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data \n",
    "dt = datetime.datetime.now()\n",
    "df.to_parquet(f'../data/processed/linkedin_jobs_{str(datetime.date(dt.year, dt.month, dt.day))}.parquet.gzip', compression='gzip')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
